{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Hook' from 'detectron2.engine' (c:\\dev\\dataset\\tomato_mrcnn\\detectron2\\engine\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39m# *******\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdetectron2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhooks\u001b[39;00m \u001b[39mimport\u001b[39;00m HookBase\n\u001b[1;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdetectron2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m Hook, DEFAULT_HOOKS, SimpleTrainer\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdetectron2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m build_detection_train_loader\n\u001b[0;32m     40\u001b[0m \u001b[39m# *******\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Hook' from 'detectron2.engine' (c:\\dev\\dataset\\tomato_mrcnn\\detectron2\\engine\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "# ต้อง ลงทะเบียน dataset ทุกครั้ง ที่ยังไม่ได้ลงทะเบียน เพื่อนำไปเทรน :D\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "register_coco_instances(\"tomato_train\", {}, \"C:/dev/dataset/tomato_mrcnn/laboro_tomato/annotations/train.json\", \"C:/dev/dataset/tomato_mrcnn/laboro_tomato/train\")\n",
    "register_coco_instances(\"tomato_test\", {}, \"C:/dev/dataset/tomato_mrcnn/laboro_tomato/annotations/test.json\", \"C:/dev/dataset/tomato_mrcnn/laboro_tomato/test\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import DatasetMapper, build_detection_test_loader\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.modeling import build_model\n",
    "import datetime\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "class ImageProcessingMapper(DatasetMapper):\n",
    "    def __init__(self, cfg, is_train=True, kernel_size=5, gamma=1.0, blur_enable=False):\n",
    "        super(ImageProcessingMapper, self).__init__(cfg, is_train)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.gamma = gamma\n",
    "        self.blur_enable = blur_enable\n",
    "\n",
    "    def apply_image_processing(self, image):\n",
    "        if self.blur_enable:\n",
    "            image = cv2.GaussianBlur(image, (self.kernel_size, self.kernel_size), 0)\n",
    "\n",
    "        lookUpTable = np.empty((1, 256), np.uint8)\n",
    "        for i in range(256):\n",
    "            lookUpTable[0, i] = np.clip(pow(i / 255.0, self.gamma) * 255.0, 0, 255)\n",
    "        return cv2.LUT(image.astype(np.uint8), lookUpTable)\n",
    "\n",
    "    def __call__(self, dataset_dict):\n",
    "        dataset_dict = super(ImageProcessingMapper, self).__call__(dataset_dict)\n",
    "        image = dataset_dict[\"image\"].numpy().transpose(1, 2, 0)\n",
    "        image = self.apply_image_processing(image)\n",
    "        dataset_dict[\"image\"] = torch.Tensor(image.transpose(2, 0, 1))\n",
    "        return dataset_dict\n",
    "\n",
    "def log_model_details(model, logger):\n",
    "    param_details = []\n",
    "    for name, param in model.named_parameters():\n",
    "        param_details.append(f\"{name}: {param.size()}\")\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    param_details.append(f'Total number of parameters: {total_params}')\n",
    "\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = total_params - trainable_params\n",
    "    param_details.append(f'Trainable parameters: {trainable_params}')\n",
    "    param_details.append(f'Non-trainable parameters: {non_trainable_params}')\n",
    "\n",
    "    for detail in param_details:\n",
    "        logger.info(detail)\n",
    "\n",
    "\n",
    "def setup_directories(model_used):\n",
    "    base_output_dir = os.path.join(\".\", \"train\", model_used)\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    specific_output_dir = os.path.join(base_output_dir, current_time)\n",
    "\n",
    "    before_test_dir = os.path.join(specific_output_dir, \"before\")\n",
    "    after_test_dir = os.path.join(specific_output_dir, \"after\")\n",
    "    os.makedirs(before_test_dir, exist_ok=True)\n",
    "    os.makedirs(after_test_dir, exist_ok=True)\n",
    "\n",
    "    log_dir = os.path.join(specific_output_dir)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    log_file_path = setup_logging(log_dir)\n",
    "    return before_test_dir, after_test_dir, log_file_path\n",
    "\n",
    "\n",
    "def setup_logging(log_dir, log_filename=\"training.log\"):\n",
    "    log_file_path = os.path.join(log_dir, log_filename)\n",
    "    logger = logging.getLogger()\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "\n",
    "    file_handler = logging.FileHandler(log_file_path)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    return log_file_path\n",
    "\n",
    "\n",
    "def configure_model(model_input, lr_, epoch_, input_size_, dv_):\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(model_input))\n",
    "    cfg.DATASETS.TRAIN = (\"tomato_train\",)\n",
    "    cfg.DATASETS.TEST = (\"tomato_test\",)\n",
    "    cfg.DATALOADER.NUM_WORKERS = 4\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_input)\n",
    "    cfg.SOLVER.IMS_PER_BATCH = 1\n",
    "    cfg.SOLVER.BASE_LR = lr_\n",
    "    cfg.SOLVER.MAX_ITER = epoch_\n",
    "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 6\n",
    "    cfg.INPUT.MIN_SIZE_TRAIN = input_size_\n",
    "    cfg.MODEL.DEVICE = dv_\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def save_config(cfg, model_base, model_used):\n",
    "    config_directory = f'./train/config/{model_base}/'\n",
    "    os.makedirs(config_directory, exist_ok=True)\n",
    "    config_path = os.path.join(config_directory, model_used)\n",
    "    with open(config_path, 'w') as f:\n",
    "        f.write(cfg.dump())\n",
    "\n",
    "\n",
    "def save_model_summary(model, model_base, model_used):\n",
    "    model_summary_directory = f'./train/model_summary/{model_base}/'\n",
    "    os.makedirs(model_summary_directory, exist_ok=True)\n",
    "    model_summary_path = os.path.join(model_summary_directory, model_used.replace('.yaml', '.txt'))\n",
    "    with open(model_summary_path, 'w') as f:\n",
    "        print(str(model), file=f)\n",
    "\n",
    "\n",
    "class Trainer(DefaultTrainer):\n",
    "    before_test_dir = None\n",
    "\n",
    "    def __init__(self, cfg, before_test_dir=None):\n",
    "        super(Trainer, self).__init__(cfg)\n",
    "        Trainer.before_test_dir = before_test_dir\n",
    "\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            output_folder = cls.before_test_dir\n",
    "        return COCOEvaluator(dataset_name, cfg, False, output_folder)\n",
    "\n",
    "def train_model(cfg, before_test_dir):\n",
    "    \"\"\"\n",
    "    Train the model using the given configuration.\n",
    "    \"\"\"\n",
    "    trainer = Trainer(cfg, before_test_dir=before_test_dir)\n",
    "    trainer.resume_or_load(resume=False)\n",
    "    trainer.train()\n",
    "\n",
    "def main():\n",
    "    model_base = \"COCO-InstanceSegmentation\"\n",
    "    model_used = \"mask_rcnn_R_50_C4_1x.yaml\"\n",
    "    epoch_ = 250\n",
    "    lr_ = 0.001\n",
    "    input_size_ = (336, 336)\n",
    "    dv_ = \"cuda\"\n",
    "    setup_step_ = 0\n",
    "    model_input = f\"{model_base}/{model_used}\"\n",
    "    blur_enable = False\n",
    "    blur_kernel_size = 3\n",
    "    gamma_correction = 1.0\n",
    "\n",
    "    before_test_dir, after_test_dir, log_file_path = setup_directories(model_used)\n",
    "    logger = logging.getLogger()\n",
    "    cfg = configure_model(model_input, lr_, epoch_, input_size_, dv_)\n",
    "    save_config(cfg, model_base, model_used)\n",
    "\n",
    "    model = build_model(cfg)\n",
    "    save_model_summary(model, model_base, model_used)\n",
    "    log_model_details(model, logger)\n",
    "    #START TRAIN BRO!!\n",
    "    start_time = time.time()\n",
    "    train_model(cfg, before_test_dir)\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_time = end_time - start_time\n",
    "    minutes = total_time // 60\n",
    "    seconds = total_time % 60\n",
    "\n",
    "    logger.info(f\"บันทึกโมเดลทดสอบเสร็จแล้วที่ : {before_test_dir}\")\n",
    "    logger.info(f\"Base Model: {model_input}\")\n",
    "    logger.info(f\"จำนวณรอบการฝึกอบรม (epoch): {epoch_}\")\n",
    "    logger.info(f\"จำนวณอัตราการเรียนรู้ (lr)  : {lr_}\")\n",
    "    logger.info(f\"ขนาดต้นทาง (input size) : {input_size_}\")\n",
    "    logger.info(f\"อุปกรณ์ที่ใช้ฝึกสอน (device): {dv_}\")\n",
    "    logger.info(f\"เวลาที่ใช้ในการฝึกสอน: {minutes} นาที {seconds:.2f} วินาที\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    torch.cuda.empty_cache()\n",
    "    main()\n",
    "    torch.cuda.empty_cache()\n",
    "    # ... (Other code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
